{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b70f2041",
      "metadata": {
        "id": "b70f2041"
      },
      "source": [
        "# 문제1.\n",
        "## 다음 분석 흐름을 참고하여 물음에 답하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86381b0e",
      "metadata": {
        "id": "86381b0e"
      },
      "source": [
        "### 분석 흐름\n",
        "\n",
        "- 필요한 라이브러리 호출\n",
        "\n",
        "- 이미지 데이터 관련 작업\n",
        "    - 이미지 데이터 전처리\n",
        "    - 이미지 데이터셋 불러오기\n",
        "    - 훈련/검증/테스트로 분리\n",
        "    - 데이터로더 정의\n",
        "    \n",
        "- 모델 정의\n",
        "\n",
        "- 옵티마이저와 손실 함수 정의\n",
        "\n",
        "- 함수 정의\n",
        "    - 모델 정확도 측정 함수\n",
        "    - 훈련 데이터셋을 이용할 모델 함수\n",
        "    - 검증 데이터셋을 이용할 모델 성능 측정 함수\n",
        "    - 모델의 학습 시간을 측정하기 위한 함수\n",
        "    \n",
        "- 모델 훈련\n",
        "\n",
        "- 모델 성능 측정"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4가지 유형의 환경 관련 사진을 분류하는 문제입니다.\n",
        "- 데이터 출처 : https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification"
      ],
      "metadata": {
        "id": "6dg7x0uK6g_u"
      },
      "id": "6dg7x0uK6g_u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7번 문제 기준 가장 성능 좋게 나온 분은 소소하게나마 스벅 깊티 보내드릴게요:)\n",
        "#### 직접 제가 모델을 돌리기 어려울 수 있으니 결과가 사라지지 않게 제출 부탁드려요!"
      ],
      "metadata": {
        "id": "HXz7794G9A1D"
      },
      "id": "HXz7794G9A1D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c54fe1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3c54fe1",
        "outputId": "6542ac3e-173d-4ed1-e111-455b695e4df0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 그대로 실행해주세요!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9a2a3e",
      "metadata": {
        "id": "ca9a2a3e"
      },
      "source": [
        "### 1. 필요한 라이브러리 호출(0.5점)\n",
        "\n",
        "- 그대로 실행해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db4f519",
      "metadata": {
        "id": "3db4f519"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as Datasets\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f155350",
      "metadata": {
        "id": "0f155350"
      },
      "source": [
        "### 2. 이미지 데이터 관련 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2616481c",
      "metadata": {
        "id": "2616481c"
      },
      "source": [
        "#### 1) 이미지 데이터 전처리(1점)\n",
        "\n",
        "- transforms.Resize() 를 이용하여 훈련 데이터와 테스트 데이터 이미지 사이즈를 자유롭게 재조정하세요(ex. (256, 256))\n",
        "\n",
        "- transforms.RandomRotation() 을 이용하여 훈련 데이터 이미지를 주어진 값 이하로 회전시키세요(ex. 5)\n",
        "\n",
        "- transforms.RandomHorizontalFlip() 을 이용하여 훈련 데이터 이미지에 특정 확률로 무작위 수평 반전을 적용하는 변환을 생성하세요(ex. 0.5)\n",
        "\n",
        "- 이 외에 추가로 전처리하고 싶은 방식이 있다면 자유롭게 추가해주세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31503aa",
      "metadata": {
        "id": "c31503aa"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                           transforms.Resize((256,256)), # 수정\n",
        "                           transforms.RandomRotation(5), # 수정\n",
        "                           transforms.RandomHorizontalFlip(0.5), # 수정\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.Resize((256,256)), # 수정\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4c74a4",
      "metadata": {
        "id": "ad4c74a4"
      },
      "source": [
        "#### 2) 이미지 데이터셋 불러오기(0.5점)\n",
        "\n",
        "- 여기서는 ImageFolder을 이용하겠습니다.\n",
        "- 아래 path를 수정하여 실행해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b829d06c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b829d06c",
        "outputId": "be91e171-730b-4f63-d1c3-2972b04fd6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2381\n",
            "250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "train_path = '/content/drive/MyDrive/data_earth/data_earth/train' # 수정\n",
        "test_path = '/content/drive/MyDrive/data_earth/data_earth/test' # 수정\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    train_path,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "    test_path,\n",
        "    transform=test_transforms\n",
        ")\n",
        "\n",
        "print(len(train_dataset)), print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c7be459",
      "metadata": {
        "id": "8c7be459"
      },
      "source": [
        "#### 3) 훈련, 검증, 테스트로 분리(1점)\n",
        "\n",
        "- VALID_RATIO 값을 자유롭게 수정하여 훈련과 검증 데이터를 분할해주세요.(ex. 0.8)\n",
        "\n",
        "- 객체 복사 중 얕은 복사(shallow copy)와 깊은 복사(deep copy)의 차이에 대해 한 문장으로 설명해주세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6q7gpOgG1ex",
        "outputId": "2f22d523-b0b0-4404-e337-868ee2069175"
      },
      "id": "k6q7gpOgG1ex",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 2381\n",
              "    Root location: /content/drive/MyDrive/data_earth/data_earth/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
              "               RandomRotation(degrees=[-5.0, 5.0], interpolation=nearest, expand=False, fill=0)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b5c027",
      "metadata": {
        "id": "d7b5c027"
      },
      "outputs": [],
      "source": [
        "# 훈련과 검증 데이터 분할\n",
        "VALID_RATIO = 0.8 # 수정\n",
        "\n",
        "n_train_examples = int(len(train_dataset) * VALID_RATIO)\n",
        "\n",
        "n_valid_examples = len(train_dataset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = torch.utils.data.random_split(train_dataset,\n",
        "                                           [n_train_examples, n_valid_examples])\n",
        "\n",
        "# 검증 데이터 전처리\n",
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4sgHHJ1IMst",
        "outputId": "6a73f621-98ed-41f4-a8ec-1e97c78b0e17"
      },
      "id": "Z4sgHHJ1IMst",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['desert', 'green_area']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기에 설명해주세요 :깊은 복사는 '실제 값'을 새로운 메모리 공간에 복사하는 것을 의미하며, 얕은 복사의 경우 주소 값을 복사한다."
      ],
      "metadata": {
        "id": "LZ5NJpwH3uFt"
      },
      "id": "LZ5NJpwH3uFt"
    },
    {
      "cell_type": "markdown",
      "id": "36506403",
      "metadata": {
        "id": "36506403"
      },
      "source": [
        "#### 4) 데이터로더 정의(0.5점)\n",
        "\n",
        "- BATCH_SIZE 를 자유롭게 수정해서 분석을 진행해주세요(ex. 64)\n",
        "  - 경험상 값이 너무 작으면 분석이 오래 걸리고, 값이 너무 크면 GPU 관련 에러가 발생합니다.\n",
        "  - 64로 진행하고 Epoch를 5로 설정했을 때 기준 GPU 사용 시 분석 시간 약 20분 소요되는 점 참고해주세요!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25696dd",
      "metadata": {
        "id": "a25696dd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 # 수정\n",
        "train_iterator = torch.utils.data.DataLoader(train_data,\n",
        "                                 shuffle = True,\n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "valid_iterator = torch.utils.data.DataLoader(valid_data,\n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "test_iterator = torch.utils.data.DataLoader(test_dataset,\n",
        "                                batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3126e9c6",
      "metadata": {
        "id": "3126e9c6"
      },
      "source": [
        "### 3. 모델 정의(1.5점)\n",
        "\n",
        "- model 객체에 배치 정규화가 적용된 VGG 모델을 불러오세요.\n",
        "  - VGG가 마음에 안드신다면 세션 시간에 다루었던 다른 사전학습된 모델을 가져오셔도 상관 없습니다!\n",
        "- 사전 훈련된 모델을 사용합니다.\n",
        "- 분류 목적을 참고하여 마지막 완전 연결층의 out_features를 변경하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ed0d04",
      "metadata": {
        "id": "34ed0d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fee4fc-cd59-4074-a351-bcbb62f60377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 106MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "model = models.vgg16(pretrained=True).to(device)  # 배치 정규화가 적용된 VGG 모델 불러오기(사전 훈련된 모델 사용)\n",
        "\n",
        "# 새로운 out_features 값으로 변경하기 위해 완전 연결층 대체\n",
        "num_classes = 4 # 분류 목적에 알맞은 클래스 수로 변경\n",
        "model.classifier[6] = nn.Linear(in_features=model.classifier[6].in_features, out_features=num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058dcd7c",
      "metadata": {
        "id": "058dcd7c"
      },
      "source": [
        "### 4. 옵티마이저와 손실 함수 정의(1.5점)\n",
        "\n",
        "- optimizer에 옵티마이저를 자유롭게 설정해주세요!(ex. optim.Adam(model.parameters(), lr = 1e-7))\n",
        "\n",
        "- criterion에 분석 목적에 알맞은 손실 함수를 자유롭게 설정해주세요!(ex. nn.CrossEntropyLoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd84678",
      "metadata": {
        "id": "6bd84678"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 1e-7)# 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()# 손실 함수\n",
        "\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898e8a5f",
      "metadata": {
        "id": "898e8a5f"
      },
      "source": [
        "### 5. 함수 정의(0.5점)\n",
        "\n",
        "- 모두 그대로 실행해주세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7f566d",
      "metadata": {
        "id": "6c7f566d"
      },
      "source": [
        "#### 1) 모델 정확도 측정 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b641cc15",
      "metadata": {
        "id": "b641cc15"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc74b6c",
      "metadata": {
        "id": "4dc74b6c"
      },
      "source": [
        "#### 2) 훈련 데이터셋을 이용할 모델 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360f0820",
      "metadata": {
        "id": "360f0820"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "    for (x, y) in iterator:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e98c38",
      "metadata": {
        "id": "c1e98c38"
      },
      "source": [
        "#### 3) 검증 데이터셋을 이용할 모델 성능 측정 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae7f39b",
      "metadata": {
        "id": "fae7f39b"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (x, y) in iterator:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b589210",
      "metadata": {
        "id": "3b589210"
      },
      "source": [
        "#### 4) 모델의 학습 시간을 측정하기 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1cd3c5",
      "metadata": {
        "id": "db1cd3c5"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366b6528",
      "metadata": {
        "id": "366b6528"
      },
      "source": [
        "### 6. 모델 훈련(1.5점)\n",
        "\n",
        "- EPOCHS를 자유롭게 수정해서 분석을 진행해주세요.\n",
        "- 모델 저장 경로를 수정해주세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RL-oEGJdODaV"
      },
      "id": "RL-oEGJdODaV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7914c6",
      "metadata": {
        "id": "8b7914c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ecbf00-7bd7-414a-f865-07f6941fb097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 9m 46s\n",
            "\tTrain Loss: 1.350 | Train Acc: 41.08%\n",
            "\t Valid. Loss: 1.374 |  Valid. Acc: 11.91%\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 1 # 수정\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.monotonic()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(),'b1' ) # 수정\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Valid. Loss: {valid_loss:.3f} |  Valid. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "862fed74",
      "metadata": {
        "id": "862fed74"
      },
      "source": [
        "### 7. 모델 성능 측정(1.5점)\n",
        "- 모델이 저장된 경로를 불러와 test set으로 모델 성능을 측정해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0daae4e2",
      "metadata": {
        "id": "0daae4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66358f3e-e568-40cc-bcdc-6bcd1ed05fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.356 | Test Acc: 18.99%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('b1')) # 모델이 저장된 경로 불러오기\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnM9lnmdG9-T"
      },
      "id": "KnM9lnmdG9-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jgd1PJoAIA8q"
      },
      "id": "Jgd1PJoAIA8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gysl6Cv8v3dw"
      },
      "source": [
        "# 문제 2\n",
        "## ResNet 구현 코드입니다.\n"
      ],
      "id": "Gysl6Cv8v3dw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBThGHj7v3dy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pdb"
      ],
      "id": "RBThGHj7v3dy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Optp9Uv3dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0a5be1-00e2-4081-e985-b73b1169aa96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#맥 유저는 \"mps:0\" 사용\n",
        "device"
      ],
      "id": "s5Optp9Uv3dz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sEjYDYnv3d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8176f2-4844-4f21-b476-e67ef34e59d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../3주차/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 114596146.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../3주차/MNIST/raw/train-images-idx3-ubyte.gz to ../3주차/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../3주차/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 106633533.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../3주차/MNIST/raw/train-labels-idx1-ubyte.gz to ../3주차/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../3주차/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31519252.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../3주차/MNIST/raw/t10k-images-idx3-ubyte.gz to ../3주차/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../3주차/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3518103.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../3주차/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../3주차/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = torchvision.datasets.MNIST('../3주차', download=True, train = True,\n",
        "                                           transform = transforms.Compose([transforms.ToTensor()]))\n",
        "test_dataset = torchvision.datasets.MNIST('../3주차', download=True, train = False,\n",
        "                                           transform = transforms.Compose([transforms.ToTensor()]))"
      ],
      "id": "_sEjYDYnv3d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uodyw-ucv3d1"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)"
      ],
      "id": "uodyw-ucv3d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryFfq2UBv3d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a2bc47-0bab-4671-dcc1-d4c1d455e7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "print(inputs.shape)"
      ],
      "id": "ryFfq2UBv3d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkgPC02dG54H"
      },
      "source": [
        "##1. 아래 그림을 참고하여 빈칸을 채워주세요."
      ],
      "id": "TkgPC02dG54H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0QPeti_G54H"
      },
      "source": [
        "![스크린샷 2023-08-16 오후 7.24.14.png](<attachment:스크린샷 2023-08-16 오후 7.24.14.png>)"
      ],
      "id": "V0QPeti_G54H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y14F0lz_v3d2"
      },
      "outputs": [],
      "source": [
        "class BottleNeck(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(d_in, momentum = 0.001, eps = 0.001)\n",
        "        self.elu = nn.ELU()\n",
        "        self.conv_1x1down = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=d_in, out_channels=64, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(64, momentum = 0.001, eps = 0.001),\n",
        "            nn.ELU())\n",
        "\n",
        "        self.conv_3x3h = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64, momentum=0.001, eps=0.001),\n",
        "            nn.ELU())\n",
        "\n",
        "        self.conv_1x1up = nn.Conv2d(in_channels=64, out_channels=d_in, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn(x)\n",
        "        out = self.elu(out)\n",
        "        out = self.conv_1x1down(out)\n",
        "        out = self.conv_3x3h(out)\n",
        "        out = self.conv_1x1up(out)\n",
        "        out += x\n",
        "\n",
        "        return out"
      ],
      "id": "Y14F0lz_v3d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQyp00xwG54I"
      },
      "source": [
        "## 2. 아래 모식도를 참고하여 forward의 빈칸을 채워주세요."
      ],
      "id": "MQyp00xwG54I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H1gD-0BG54I"
      },
      "source": [
        "![KakaoTalk_Photo_2023-08-16-21-20-14.png](attachment:KakaoTalk_Photo_2023-08-16-21-20-14.png)"
      ],
      "id": "8H1gD-0BG54I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JlNVKgYv3d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, d_in=1, d_out=10, d_middle=128):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.embedding = nn.Conv2d(d_in, d_middle, 1)\n",
        "\n",
        "        self.layer1 = BottleNeck(d_middle)\n",
        "        self.layer2 = BottleNeck(d_middle)\n",
        "        self.layer3 = BottleNeck(d_middle)\n",
        "        self.layer4 = BottleNeck(d_middle)\n",
        "\n",
        "        self.to_out = nn.Linear(d_middle*28*28, d_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        return out"
      ],
      "id": "5JlNVKgYv3d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-OLLBxrv3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e20116-53a5-41e1-aae8-2a2a45afa3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (embedding): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (layer1): BottleNeck(\n",
            "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (conv_1x1down): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_3x3h): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_1x1up): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (layer2): BottleNeck(\n",
            "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (conv_1x1down): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_3x3h): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_1x1up): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (layer3): BottleNeck(\n",
            "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (conv_1x1down): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_3x3h): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_1x1up): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (layer4): BottleNeck(\n",
            "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (conv_1x1down): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_3x3h): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=0.001, momentum=0.001, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "    (conv_1x1up): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (to_out): Linear(in_features=100352, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(1,10).to(device)\n",
        "print(model)"
      ],
      "id": "3-OLLBxrv3d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0b3Ycr4v3d3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "id": "V0b3Ycr4v3d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jWI_tR4v3d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9182604-94e3-41eb-c6eb-b2443adf928f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch  0\n",
            "\n",
            "Train:\n",
            "current_loss =  5.6277852058410645\n",
            "current_loss =  9.685074806213379\n",
            "current_loss =  8.74429702758789\n",
            "current_loss =  5.84733772277832\n",
            "current_loss =  6.9178690910339355\n",
            "current_loss =  9.735209465026855\n",
            "\n",
            "Validation\n",
            "# epoch = 1, loss : 15.26620030534371, Accuracy: 89.98999786376953%\n",
            "\n",
            "Epoch  1\n",
            "\n",
            "Train:\n",
            "current_loss =  1.023603916168213\n",
            "current_loss =  1.0478816032409668\n",
            "current_loss =  4.315898418426514\n",
            "current_loss =  12.419900894165039\n",
            "current_loss =  3.8807895183563232\n",
            "current_loss =  15.031715393066406\n",
            "\n",
            "Validation\n",
            "# epoch = 2, loss : 4.799834499882336, Accuracy: 89.62999725341797%\n",
            "\n",
            "Epoch  2\n",
            "\n",
            "Train:\n",
            "current_loss =  0.23985549807548523\n",
            "current_loss =  1.4077471494674683\n",
            "current_loss =  5.829556941986084\n",
            "current_loss =  0.333763986825943\n",
            "current_loss =  5.765289783477783\n",
            "current_loss =  6.612700939178467\n",
            "\n",
            "Validation\n",
            "# epoch = 3, loss : 4.092841422622717, Accuracy: 95.55999755859375%\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "epochs = 3\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "num_epoch = 0\n",
        "epoch_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch \", epoch)\n",
        "    # train\n",
        "    print(\"\\nTrain:\")\n",
        "    ave_loss = 0\n",
        "    cnt = 0\n",
        "    for batch, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        train = Variable(inputs.view(100, 1, 28, 28))\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cnt = cnt + 1\n",
        "        if cnt % 100 == 0:\n",
        "            print('current_loss = ', loss.item())\n",
        "        ave_loss += loss.item() / len(train_loader)\n",
        "\n",
        "    num_epoch += 1\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    print(\"\\nValidation\")\n",
        "    for tests, labels in test_loader:\n",
        "        test, labels = tests.to(device), labels.to(device)\n",
        "        labels_list.append(labels)\n",
        "        outputs = model(test)\n",
        "        pred = torch.argmax(outputs, 1)\n",
        "        predictions_list.append(pred)\n",
        "        correct += (pred == labels).sum()\n",
        "        total += len(labels)\n",
        "\n",
        "    accuracy = correct * 100 / total\n",
        "    epoch_list.append(num_epoch)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print(\"# epoch = {}, loss : {}, Accuracy: {}%\".format(num_epoch, ave_loss, accuracy))\n"
      ],
      "id": "7jWI_tR4v3d4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aQ1PTkNDJIb"
      },
      "id": "7aQ1PTkNDJIb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}